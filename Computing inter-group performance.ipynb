{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fb0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "import pickle\n",
    "import torch\n",
    "def defaultdict_to_dict(d):\n",
    "    \"\"\" Recursively convert defaultdict to dict. \"\"\"\n",
    "    if isinstance(d, defaultdict):\n",
    "        d = {key: defaultdict_to_dict(value) for key, value in d.items()}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786849ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_files_with_str(directory, str_contain, index):\n",
    "    # Create an empty list to store files that contain str_contain\n",
    "    files_with_str = []\n",
    "\n",
    "    # Iterate over the files in the directory\n",
    "    for file in os.listdir(directory):\n",
    "        if file[:4] == str_contain:\n",
    "            files_with_str.append(file)\n",
    "    \n",
    "    \n",
    "    # Sort only the files that contain 'x1'\n",
    "    files_with_str.sort()\n",
    "\n",
    "    # Return the first file in the sorted list, or None if the list is empty\n",
    "    return files_with_str[index] if files_with_str else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0865bbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 8090 8090_2023-12-16 175932_my_t5_model\n",
      "1 8091 8091_2023-12-16 180341_my_t5_model\n",
      "1 8092 8092_2023-12-16 180103_my_t5_model\n",
      "1 8093 8093_2023-12-16 191901_my_t5_model\n",
      "1 8094 8094_2023-12-16 180320_my_t5_model\n",
      "1 8095 8095_2023-12-16 180348_my_t5_model\n",
      "1 8096 8096_2023-12-16 180129_my_t5_model\n",
      "1 8097 8097_2023-12-16 180007_my_t5_model\n",
      "1 8098 8098_2023-12-16 191911_my_t5_model\n",
      "1 8099 8099_2023-12-16 191700_my_t5_model\n",
      "2 8090 8090_2023-12-17 123459_my_t5_model\n",
      "2 8091 8091_2023-12-17 123536_my_t5_model\n",
      "2 8092 8092_2023-12-17 123202_my_t5_model\n",
      "2 8093 8093_2023-12-17 123152_my_t5_model\n",
      "2 8094 8094_2023-12-17 124010_my_t5_model\n",
      "2 8095 8095_2023-12-17 123723_my_t5_model\n",
      "2 8096 8096_2023-12-17 140032_my_t5_model\n",
      "2 8097 8097_2023-12-17 123518_my_t5_model\n",
      "2 8098 8098_2023-12-17 123711_my_t5_model\n",
      "2 8099 8099_2023-12-17 123633_my_t5_model\n",
      "3 8090 8090_2023-12-18 082034_my_t5_model\n",
      "3 8091 8091_2023-12-18 070742_my_t5_model\n",
      "3 8092 8092_2023-12-18 070621_my_t5_model\n",
      "3 8093 8093_2023-12-18 081950_my_t5_model\n",
      "3 8094 8094_2023-12-18 071001_my_t5_model\n",
      "3 8095 8095_2023-12-18 070540_my_t5_model\n",
      "3 8096 8096_2023-12-18 070510_my_t5_model\n",
      "3 8097 8097_2023-12-18 070723_my_t5_model\n",
      "3 8098 8098_2023-12-18 070705_my_t5_model\n",
      "3 8099 8099_2023-12-18 070413_my_t5_model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Reading data and models\n",
    "groups = [str(i) for i in range(1,4)]\n",
    "peers = [str(i) for i in range(8090, 8100)]\n",
    "\n",
    "#reading entire dataset for all groups: train_df_group1\n",
    "\n",
    "\n",
    "\n",
    "# for group in groups:\n",
    "#     datasets_folder = os.path.join('aggregated_results',f'10_peers_10kEpochs_group{group}','datasets')\n",
    "#     personal_dfs = os.listdir(datasets_folder)\n",
    "#     personal_dfs = [i for i in personal_dfs if '.csv' in i and 'train' not in i and 'test' not in i]\n",
    "#     print (personal_dfs)\n",
    "#     for i in personal_dfs:\n",
    "#         df_temp = pd.read_csv(os.path.join(datasets_folder, i))\n",
    "#         print (df_temp['doc_id'].nunique())\n",
    "#     exec(f'train_df_group{group} = train_df_group{group}.drop_duplicates()')\n",
    "    \n",
    "    \n",
    "# reading individual peer datasets & group datasets: train_df_group1, train_df_group1_peer1\n",
    "for group in groups:\n",
    "    # creating train_df's\n",
    "    exec(f'train_df_group{group} = pd.DataFrame()')\n",
    "    for peer in peers:\n",
    "        datasets_folder = os.path.join('aggregated_results',f'10_peers_10kEpochs_group{group}','datasets')\n",
    "        exec_str = f\"train_df_group{group}_peer{int(peer) - 8089} = pd.read_csv(os.path.join(datasets_folder,'{peer}_df.csv'))\"\n",
    "        exec(exec_str)\n",
    "        exec(f'train_df_group{group} = pd.concat([train_df_group{group}, train_df_group{group}_peer{int(peer) - 8089}])')\n",
    "       \n",
    "    exec(f'train_df_group{group} = train_df_group{group}.drop_duplicates()')\n",
    "    \n",
    "    # creating test_df's: test_df_group1, test_df_group1_peer1\n",
    "    \n",
    "    datasets_folder = os.path.join('aggregated_results',f'10_peers_10kEpochs_group{group}','datasets')\n",
    "    exec (f\"test_df_group{group} = pd.read_csv(os.path.join(datasets_folder,'test_df.csv')) \")\n",
    "    exec (f\"test_df_group{group} = test_df_group{group}[test_df_group{group}['doc_id'].isin(train_df_group{group}['doc_id'].unique())]\")\n",
    "    for peer in peers:\n",
    "        exec (f\"test_df_group{group}_peer{int(peer) - 8089} = test_df_group{group}[test_df_group{group}['doc_id'].isin(train_df_group{group}_peer{int(peer) - 8089}['doc_id'].unique())]\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#reading models: model_group1_peer1\n",
    "for group in groups:\n",
    "    for peer in peers:\n",
    "        model_folder = os.path.join('aggregated_results',f'10_peers_10kEpochs_group{group}', 'models')\n",
    "        model_file = find_first_files_with_str(model_folder, peer, 10) # 10 is the largest number of saved models that all peers have finished training\n",
    "        print (group, peer, model_file)\n",
    "        exec_str = f\"model_group{group}_peer{str(int(peer)-8089)} = T5ForConditionalGeneration.from_pretrained(os.path.join(model_folder, model_file))\"\n",
    "        \n",
    "        \n",
    "        exec(exec_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b5fc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "2001\n",
      "2032\n",
      "2002\n",
      "test\n",
      "2001\n",
      "2032\n",
      "2002\n"
     ]
    }
   ],
   "source": [
    "print ('train')\n",
    "print (train_df_group1['doc_id'].nunique())\n",
    "print (train_df_group2['doc_id'].nunique())\n",
    "print (train_df_group3['doc_id'].nunique())\n",
    "print ('test')\n",
    "print (test_df_group1['doc_id'].nunique())\n",
    "print (test_df_group2['doc_id'].nunique())\n",
    "print (test_df_group3['doc_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208dd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d55c8c6",
   "metadata": {},
   "source": [
    "### CALCULATING ACCURACIES ON MODELS' OWN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fde6e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model_and_evaluate(group, peer, mode = 'global'):\n",
    "    global_scope = globals()\n",
    "    acc_train = -1\n",
    "    print ('group ', group, 'peer ', peer)\n",
    "    exec (f'model = model_group{group}_peer{peer}', global_scope)\n",
    "    if mode == 'global':\n",
    "        exec (f'train_df = train_df_group{group}.sample(1000).copy()', global_scope)\n",
    "        exec (f'test_df = test_df_group{group}.sample(1000).copy()', global_scope)\n",
    "    elif mode == 'local':\n",
    "        exec (f'train_df = train_df_group{group}_peer{peer}.sample(1000).copy()', global_scope)\n",
    "        exec (f'test_df = test_df_group{group}_peer{peer}.sample(1000).copy()', global_scope)\n",
    "        \n",
    "    df_tot = train_df.copy()\n",
    "    df_tst = test_df.copy()\n",
    "    print (df_tot.shape, df_tst.shape)\n",
    "    \n",
    "    df_tot['generated_doc_id'] = df_tot['query'].apply(lambda x: generate_text(x, model))\n",
    "    df_tst['generated_doc_id'] = df_tst['query'].apply(lambda x: generate_text(x, model))\n",
    "    acc_train = df_tot[df_tot['doc_id'] == df_tot['generated_doc_id']].shape[0]/df_tot.shape[0]\n",
    "    acc_test = df_tst[df_tst['doc_id'] == df_tst['generated_doc_id']].shape[0]/df_tst.shape[0]\n",
    "    \n",
    "    \n",
    "    print (f'{mode} training set accuracy: ', acc_train)\n",
    "    print (f'{mode} test set accuracy: ', acc_test)\n",
    "    \n",
    "    \n",
    "    df_tot['generated_doc_id_log'] = df_tot['query'].apply(lambda x: generate_text_through_logits(x, model, df_tot))\n",
    "    df_tst['generated_doc_id_log'] = df_tst['query'].apply(lambda x: generate_text_through_logits(x, model, df_tst))\n",
    "\n",
    "    \n",
    "    acc_train_log = df_tot[df_tot['doc_id'] == df_tot['generated_doc_id_log']].shape[0]/df_tot.shape[0]\n",
    "    acc_test_log = df_tst[df_tst['doc_id'] == df_tst['generated_doc_id_log']].shape[0]/df_tst.shape[0]\n",
    "    \n",
    "    print (f'{mode} training set accuracy log: ', acc_train_log)\n",
    "    print (f'{mode} test set accuracy log: ', acc_test_log)\n",
    "    return acc_train, acc_test, df_tot, df_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9bc08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_text(query, model):\n",
    "    input_ids = tokenizer.encode(query, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length = 20)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe58e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_through_logits(query,model, df_tst):\n",
    "    doc_id = df_tst[df_tst['query'] == query]['doc_id'].iloc[0]\n",
    "#     print (query, doc_id)\n",
    "    inputs = tokenizer(query, padding=False, return_tensors=\"pt\", truncation=True).input_ids\n",
    "    labels = tokenizer(doc_id, padding=True, return_tensors=\"pt\", truncation=True).input_ids\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=inputs, labels = labels)\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # Extract logits and convert to token IDs\n",
    "    logits = outputs.logits\n",
    "    predicted_token_ids = tokenizer.decode(torch.argmax(logits, dim=-1)[0], skip_special_tokens=True)\n",
    "#     print (predicted_token_ids)\n",
    "    return predicted_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "328a3ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group  1 peer  1\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.981\n",
      "local test set accuracy:  0.909\n",
      "local training set accuracy log:  0.981\n",
      "local test set accuracy log:  0.909\n",
      "group  1 peer  1\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.977\n",
      "global test set accuracy:  0.884\n",
      "global training set accuracy log:  0.977\n",
      "global test set accuracy log:  0.884\n",
      "group  1 peer  2\n",
      "(1000, 5) (1000, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_accuracies = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(float))))\n",
    "for group in groups:\n",
    "    for peer in peers:\n",
    "        \n",
    "        p = int(peer) - 8089\n",
    "        exec(f\"acc_train_local, acc_test_local, df_tot_l, df_tst_l = read_model_and_evaluate({group}, {p}, 'local')\")\n",
    "        global_accuracies[group][peer]['train']['local'] = acc_train_local\n",
    "        global_accuracies[group][peer]['test']['local'] = acc_test_local\n",
    "        \n",
    "        exec(f\"acc_train_global, acc_test_global, df_tot_g, df_tst_g = read_model_and_evaluate({group}, {p}, 'global')\")\n",
    "        global_accuracies[group][peer]['train']['global'] = acc_train_global\n",
    "        global_accuracies[group][peer]['test']['global'] = acc_test_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffc90937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>doc</th>\n",
       "      <th>generated_doc_id</th>\n",
       "      <th>generated_doc_id_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>12967572</td>\n",
       "      <td>9682982</td>\n",
       "      <td>quality control duties</td>\n",
       "      <td>D2450576</td>\n",
       "      <td>https://www.bls.gov/ooh/production/quality-con...</td>\n",
       "      <td>D2531578</td>\n",
       "      <td>D25450576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>14534584</td>\n",
       "      <td>11604881</td>\n",
       "      <td>sss math</td>\n",
       "      <td>D235655</td>\n",
       "      <td>http://mathopenref.com/congruentsss.html</td>\n",
       "      <td>D1667190</td>\n",
       "      <td>D165655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552</th>\n",
       "      <td>2151563</td>\n",
       "      <td>4382814</td>\n",
       "      <td>bouteflika</td>\n",
       "      <td>D1960677</td>\n",
       "      <td>https://www.britannica.com/biography/Abdelaziz...</td>\n",
       "      <td>D1969822</td>\n",
       "      <td>D1969877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>2609542</td>\n",
       "      <td>2624344</td>\n",
       "      <td>car loans</td>\n",
       "      <td>D29479</td>\n",
       "      <td>https://santanderconsumerusa.com/</td>\n",
       "      <td>D1242700</td>\n",
       "      <td>D129479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>14531330</td>\n",
       "      <td>3752354</td>\n",
       "      <td>ssi/ssdi benefits</td>\n",
       "      <td>D2197809</td>\n",
       "      <td>https://www.ssa.gov/disability/disability.html</td>\n",
       "      <td>D610732</td>\n",
       "      <td>D6197809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>13559621</td>\n",
       "      <td>3452459</td>\n",
       "      <td>russian federation countries</td>\n",
       "      <td>D1664655</td>\n",
       "      <td>https://history.state.gov/countries/russia</td>\n",
       "      <td>D1664542</td>\n",
       "      <td>D1664555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>16111839</td>\n",
       "      <td>2906342</td>\n",
       "      <td>united airlines booking number</td>\n",
       "      <td>D2790515</td>\n",
       "      <td>http://www.bookmyflightticket.com/flights/unit...</td>\n",
       "      <td>D2791406</td>\n",
       "      <td>D2791415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>17588713</td>\n",
       "      <td>10309374</td>\n",
       "      <td>what is social security disability income</td>\n",
       "      <td>D610732</td>\n",
       "      <td>https://www.disabilitysecrets.com/page5-13.html</td>\n",
       "      <td>D2197809</td>\n",
       "      <td>D210732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>14295142</td>\n",
       "      <td>5600135</td>\n",
       "      <td>social security vs disability</td>\n",
       "      <td>D610732</td>\n",
       "      <td>https://www.disabilitysecrets.com/page5-13.html</td>\n",
       "      <td>D2197809</td>\n",
       "      <td>D210732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>18809142</td>\n",
       "      <td>3630454</td>\n",
       "      <td>Ð½Ð¾ÑÐ²ÐµÐ³Ð¸Ñ</td>\n",
       "      <td>D176337</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Norway</td>\n",
       "      <td>D2085252</td>\n",
       "      <td>D2086337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>18155117</td>\n",
       "      <td>7331703</td>\n",
       "      <td>wiki</td>\n",
       "      <td>D176337</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Norway</td>\n",
       "      <td>D1197835</td>\n",
       "      <td>D116337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>3888580</td>\n",
       "      <td>12288734</td>\n",
       "      <td>csm project management</td>\n",
       "      <td>D1067347</td>\n",
       "      <td>https://www.simplilearn.com/benefits-of-csm-ce...</td>\n",
       "      <td>D3491861</td>\n",
       "      <td>D343680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>9688561</td>\n",
       "      <td>2396113</td>\n",
       "      <td>little mina</td>\n",
       "      <td>D1012688</td>\n",
       "      <td>http://visitmena.com/</td>\n",
       "      <td>D1010077</td>\n",
       "      <td>D1010088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>10554955</td>\n",
       "      <td>12513979</td>\n",
       "      <td>mike webster nfl</td>\n",
       "      <td>D3218960</td>\n",
       "      <td>http://www.pbs.org/wgbh/pages/frontline/oral-h...</td>\n",
       "      <td>D27182</td>\n",
       "      <td>D27888960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>12170175</td>\n",
       "      <td>2884993</td>\n",
       "      <td>pdm fema</td>\n",
       "      <td>D1476598</td>\n",
       "      <td>https://www.fema.gov/pre-disaster-mitigation-g...</td>\n",
       "      <td>D383240</td>\n",
       "      <td>D386598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>3789515</td>\n",
       "      <td>11610042</td>\n",
       "      <td>craigslist.com</td>\n",
       "      <td>D1832805</td>\n",
       "      <td>https://onslow.craigslist.org/</td>\n",
       "      <td>D577820</td>\n",
       "      <td>D572805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>4824913</td>\n",
       "      <td>12474044</td>\n",
       "      <td>drivers</td>\n",
       "      <td>D432626</td>\n",
       "      <td>http://driverrestore.com/</td>\n",
       "      <td>D1394306</td>\n",
       "      <td>D1392626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>11095157</td>\n",
       "      <td>10229057</td>\n",
       "      <td>native american headress</td>\n",
       "      <td>D3456910</td>\n",
       "      <td>http://indians.org/articles/native-american-he...</td>\n",
       "      <td>D1394306</td>\n",
       "      <td>D139456910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>7451964</td>\n",
       "      <td>9530489</td>\n",
       "      <td>how long do shih tzus live</td>\n",
       "      <td>D211044</td>\n",
       "      <td>https://answers.yahoo.com/question/index?qid=2...</td>\n",
       "      <td>D2190494</td>\n",
       "      <td>D219044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>11552225</td>\n",
       "      <td>7757334</td>\n",
       "      <td>nyc jazz record</td>\n",
       "      <td>D1383674</td>\n",
       "      <td>http://nycjazzrecord.com/</td>\n",
       "      <td>D1387348</td>\n",
       "      <td>D1387374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>14728318</td>\n",
       "      <td>8910611</td>\n",
       "      <td>straddling definition</td>\n",
       "      <td>D2199724</td>\n",
       "      <td>https://www.urbandictionary.com/define.php?ter...</td>\n",
       "      <td>D3297063</td>\n",
       "      <td>D3199724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  query_id                                      query  \\\n",
       "162     12967572   9682982                     quality control duties   \n",
       "1964    14534584  11604881                                   sss math   \n",
       "2552     2151563   4382814                                 bouteflika   \n",
       "893      2609542   2624344                                  car loans   \n",
       "555     14531330   3752354                          ssi/ssdi benefits   \n",
       "1586    13559621   3452459               russian federation countries   \n",
       "855     16111839   2906342             united airlines booking number   \n",
       "231     17588713  10309374  what is social security disability income   \n",
       "998     14295142   5600135              social security vs disability   \n",
       "734     18809142   3630454                             Ð½Ð¾ÑÐ²ÐµÐ³Ð¸Ñ   \n",
       "1544    18155117   7331703                                       wiki   \n",
       "1912     3888580  12288734                     csm project management   \n",
       "916      9688561   2396113                                little mina   \n",
       "204     10554955  12513979                           mike webster nfl   \n",
       "1053    12170175   2884993                                   pdm fema   \n",
       "1406     3789515  11610042                             craigslist.com   \n",
       "1850     4824913  12474044                                    drivers   \n",
       "1435    11095157  10229057                   native american headress   \n",
       "469      7451964   9530489                 how long do shih tzus live   \n",
       "596     11552225   7757334                            nyc jazz record   \n",
       "1634    14728318   8910611                      straddling definition   \n",
       "\n",
       "        doc_id                                                doc  \\\n",
       "162   D2450576  https://www.bls.gov/ooh/production/quality-con...   \n",
       "1964   D235655           http://mathopenref.com/congruentsss.html   \n",
       "2552  D1960677  https://www.britannica.com/biography/Abdelaziz...   \n",
       "893     D29479                  https://santanderconsumerusa.com/   \n",
       "555   D2197809     https://www.ssa.gov/disability/disability.html   \n",
       "1586  D1664655         https://history.state.gov/countries/russia   \n",
       "855   D2790515  http://www.bookmyflightticket.com/flights/unit...   \n",
       "231    D610732    https://www.disabilitysecrets.com/page5-13.html   \n",
       "998    D610732    https://www.disabilitysecrets.com/page5-13.html   \n",
       "734    D176337               https://en.wikipedia.org/wiki/Norway   \n",
       "1544   D176337               https://en.wikipedia.org/wiki/Norway   \n",
       "1912  D1067347  https://www.simplilearn.com/benefits-of-csm-ce...   \n",
       "916   D1012688                              http://visitmena.com/   \n",
       "204   D3218960  http://www.pbs.org/wgbh/pages/frontline/oral-h...   \n",
       "1053  D1476598  https://www.fema.gov/pre-disaster-mitigation-g...   \n",
       "1406  D1832805                     https://onslow.craigslist.org/   \n",
       "1850   D432626                          http://driverrestore.com/   \n",
       "1435  D3456910  http://indians.org/articles/native-american-he...   \n",
       "469    D211044  https://answers.yahoo.com/question/index?qid=2...   \n",
       "596   D1383674                          http://nycjazzrecord.com/   \n",
       "1634  D2199724  https://www.urbandictionary.com/define.php?ter...   \n",
       "\n",
       "     generated_doc_id generated_doc_id_log  \n",
       "162          D2531578            D25450576  \n",
       "1964         D1667190              D165655  \n",
       "2552         D1969822             D1969877  \n",
       "893          D1242700              D129479  \n",
       "555           D610732             D6197809  \n",
       "1586         D1664542             D1664555  \n",
       "855          D2791406             D2791415  \n",
       "231          D2197809              D210732  \n",
       "998          D2197809              D210732  \n",
       "734          D2085252             D2086337  \n",
       "1544         D1197835              D116337  \n",
       "1912         D3491861              D343680  \n",
       "916          D1010077             D1010088  \n",
       "204            D27182            D27888960  \n",
       "1053          D383240              D386598  \n",
       "1406          D577820              D572805  \n",
       "1850         D1394306             D1392626  \n",
       "1435         D1394306           D139456910  \n",
       "469          D2190494              D219044  \n",
       "596          D1387348             D1387374  \n",
       "1634         D3297063             D3199724  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tot_g[df_tot_g['generated_doc_id'] != df_tot_g['generated_doc_id_log']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5222845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('localnglobal_accuracies_allgroups_allpeers.pkl', 'wb') as file:\n",
    "    dump(global_accuracies, file)\n",
    "\n",
    "# # Step 4: Load from the file\n",
    "# with open('my_defaultdict.pkl', 'rb') as file:\n",
    "#     loaded_defaultdict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a643fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('localnglobal_accuracies_allgroups_allpeers.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)\n",
    "\n",
    "# To load and optionally convert back to defaultdict\n",
    "# (You'll need to redefine your defaultdict structure as before)\n",
    "with open('localnglobal_accuracies_allgroups_allpeers.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d03e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load and optionally convert back to defaultdict\n",
    "# (You'll need to redefine your defaultdict structure as before)\n",
    "with open('localnglobal_accuracies_allgroups_allpeers.pkl', 'rb') as file:\n",
    "    global_accuracies = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56730e20",
   "metadata": {},
   "source": [
    "### CALCULATING ACCURACIES ON TOP5 FOR EACH PEER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_group1.shape,test_df_group2.shape,test_df_group3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5c94c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groupgroup 1 peer 2 mode global\n",
      "group 1 peer 3 mode global\n",
      " 1 peer 1 mode global\n",
      "group 1 peer 4 mode global\n",
      "group 1 peer 5 mode global\n",
      "group 1 peer 6 mode global\n",
      "(16432, 5) (16464, 5)\n",
      "(16432, 5) (16464, 5)\n",
      "(16432, 5) (16464, 5)\n",
      "group 1 peer 7 mode global\n",
      "(16432, 5) (16464, 5)\n",
      "(16432, 5) (16464, 5)\n",
      "(16432, 5) (16464, 5)\n",
      "(16432, 5) (16464, 5)\n",
      "group 1 peer 8 mode global\n",
      "group 1 peer 9 mode global\n",
      "group 1 peer 10 mode global\n",
      "(16432, 5) (16464, 5)\n",
      "(16432, 5) (16464, 5)\n",
      "(16432, 5) (16464, 5)\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 4000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 5000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 5000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 6000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 6000 queries\n",
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 7000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 8000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 8000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 11000 queries\n",
      "Processed 10000 queries\n",
      "Processed 9000 queries\n",
      "Processed 11000 queries\n",
      "Processed 9000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 12000 queries\n",
      "Processed 11000 queries\n",
      "Processed 12000 queries\n",
      "Processed 10000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 10000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 12000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 11000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 11000 queries\n",
      "Processed 14000 queries\n",
      "Processed 13000 queries\n",
      "Processed 14000 queries\n",
      "Processed 13000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 12000 queries\n",
      "Processed 14000 queries\n",
      "Processed 15000 queries\n",
      "Processed 14000 queries\n",
      "Processed 12000 queries\n",
      "Processed 15000 queries\n",
      "Processed 14000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 16000 queries\n",
      "Processed 13000 queries\n",
      "Processed 15000 queries\n",
      "Processed 16000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9277818270165209\n",
      "finished global work for group 1 and peer 1, acc test global :0.9277818270165209\n",
      "Processed 13000 queries\n",
      "Processed 15000 queries\n",
      "Processed 16000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9149659863945578\n",
      "finished global work for group 1 and peer 7, acc test global :0.9149659863945578\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.8914601554907677\n",
      "finished global work for group 1 and peer 10, acc test global :0.8914601554907677\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9186710398445093\n",
      "finished global work for group 1 and peer 3, acc test global :0.9186710398445093\n",
      "Processed 16000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9234086491739553\n",
      "finished global work for group 1 and peer 8, acc test global :0.9234086491739553\n",
      "Processed 14000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.924198250728863\n",
      "finished global work for group 1 and peer 9, acc test global :0.924198250728863\n",
      "Processed 16000 queries\n",
      "Processed 14000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9241375121477162\n",
      "finished global work for group 1 and peer 5, acc test global :0.9241375121477162\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9289358600583091\n",
      "finished global work for group 1 and peer 6, acc test global :0.9289358600583091\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9233479105928085\n",
      "finished global work for group 1 and peer 4, acc test global :0.9233479105928085\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9275388726919339\n",
      "finished global work for group 1 and peer 2, acc test global :0.9275388726919339\n",
      "group 2 peer 1 mode global\n",
      "group 2 peer 2 mode global\n",
      "group 2 peer 3 mode global\n",
      "(17311, 5) (17329, 5)\n",
      "group 2 peer 4 mode global\n",
      "(17311, 5) (17329, 5)\n",
      "group 2 peer 5 mode global\n",
      "(17311, 5) (17329, 5)\n",
      "(17311, 5) (17329, 5)\n",
      "group 2 peer 6 mode global\n",
      "(17311, 5) (17329, 5)\n",
      "group 2 peer 7 mode global\n",
      "(17311, 5) (17329, 5)\n",
      "group 2 peer 8 mode global\n",
      "group 2 peer 9 mode global\n",
      "(17311, 5) (17329, 5)\n",
      "(17311, 5) (17329, 5)\n",
      "(17311, 5) (17329, 5)\n",
      "group 2 peer 10 mode global\n",
      "(17311, 5) (17329, 5)\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 5000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 6000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 9000 queries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 8000 queries\n",
      "Processed 9000 queries\n",
      "Processed 10000 queries\n",
      "Processed 9000 queries\n",
      "Processed 10000 queries\n",
      "Processed 9000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 11000 queries\n",
      "Processed 10000 queries\n",
      "Processed 9000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 11000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 10000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 12000 queries\n",
      "Processed 13000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 11000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 14000 queries\n",
      "Processed 12000 queries\n",
      "Processed 14000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 14000 queries\n",
      "Processed 15000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 13000 queries\n",
      "Processed 16000 queries\n",
      "Processed 15000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "Processed 15000 queries\n",
      "Processed 16000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 17000 queries\n",
      "Processed 17000 queries\n",
      "Processed 16000 queries\n",
      "Processed 17000 queries\n",
      "Processed 14000 queries\n",
      "Processed 17000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9123434704830053\n",
      "finished global work for group 2 and peer 8, acc test global :0.9123434704830053\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9195568122799931\n",
      "finished global work for group 2 and peer 1, acc test global :0.9195568122799931\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9211148941081424\n",
      "finished global work for group 2 and peer 9, acc test global :0.9211148941081424\n",
      "Processed 16000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9140746725142824\n",
      "finished global work for group 2 and peer 5, acc test global :0.9140746725142824\n",
      "Processed 17000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9186912112643546\n",
      "finished global work for group 2 and peer 10, acc test global :0.9186912112643546\n",
      "Processed 17000 queries\n",
      "Processed 15000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9158058745455595\n",
      "finished global work for group 2 and peer 3, acc test global :0.9158058745455595\n",
      "Processed 17000 queries\n",
      "Processed 17000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9220382018581569\n",
      "finished global work for group 2 and peer 6, acc test global :0.9220382018581569\n",
      "Processed 17000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9147671533267933\n",
      "finished global work for group 2 and peer 2, acc test global :0.9147671533267933\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9133821917017716\n",
      "finished global work for group 2 and peer 4, acc test global :0.9133821917017716\n",
      "Processed 16000 queries\n",
      "Processed 17000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9242310577644411\n",
      "finished global work for group 2 and peer 7, acc test global :0.9242310577644411\n",
      "group 3 peer 1 mode global\n",
      "group 3 peer 2 mode global\n",
      "group 3 peer 3 mode global\n",
      "(16647, 5) (16674, 5)\n",
      "(16647, 5) (16674, 5)\n",
      "(16647, 5) (16674, 5)\n",
      "group 3 peer 4 mode global\n",
      "(16647, 5) (16674, 5)\n",
      "group 3 peer 5 mode global\n",
      "(16647, 5) (16674, 5)\n",
      "group 3 peer 6 mode global\n",
      "group 3 peer 7 mode global\n",
      "group 3 peer 8 mode global\n",
      "(16647, 5) (16674, 5)\n",
      "(16647, 5) (16674, 5)\n",
      "group 3 peer 9 mode global\n",
      "group 3 peer 10 mode global\n",
      "(16647, 5) (16674, 5)\n",
      "(16647, 5) (16674, 5)\n",
      "(16647, 5) (16674, 5)\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 5000 queries\n",
      "Processed 4000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 5000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 7000 queries\n",
      "Processed 6000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 6000 queries\n",
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 7000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 8000 queries\n",
      "Processed 9000 queries\n",
      "Processed 8000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 10000 queries\n",
      "Processed 8000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 9000 queries\n",
      "Processed 10000 queries\n",
      "Processed 9000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 9000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 10000 queries\n",
      "Processed 11000 queries\n",
      "Processed 10000 queries\n",
      "Processed 12000 queries\n",
      "Processed 11000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 10000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 13000 queries\n",
      "Processed 12000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 11000 queries\n",
      "Processed 13000 queries\n",
      "Processed 14000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 13000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 15000 queries\n",
      "Processed 12000 queries\n",
      "Processed 13000 queries\n",
      "Processed 14000 queries\n",
      "Processed 13000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 16000 queries\n",
      "Processed 14000 queries\n",
      "Processed 13000 queries\n",
      "Processed 16000 queries\n",
      "Processed 15000 queries\n",
      "Processed 16000 queries\n",
      "Processed 14000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9198152812762385\n",
      "finished global work for group 3 and peer 3, acc test global :0.9198152812762385\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9313901883171405\n",
      "finished global work for group 3 and peer 1, acc test global :0.9313901883171405\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9316900563751949\n",
      "finished global work for group 3 and peer 9, acc test global :0.9316900563751949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9186757826556315\n",
      "finished global work for group 3 and peer 7, acc test global :0.9186757826556315\n",
      "Processed 15000 queries\n",
      "Processed 16000 queries\n",
      "Processed 14000 queries\n",
      "Processed 15000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9274919035624325\n",
      "finished global work for group 3 and peer 10, acc test global :0.9274919035624325\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9314501619287513\n",
      "finished global work for group 3 and peer 6, acc test global :0.9314501619287513\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9292911119107593\n",
      "finished global work for group 3 and peer 2, acc test global :0.9292911119107593\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "Processed 15000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9319899244332494\n",
      "finished global work for group 3 and peer 4, acc test global :0.9319899244332494\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9334292911119108\n",
      "finished global work for group 3 and peer 8, acc test global :0.9334292911119108\n",
      "Processed 16000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9256926952141058\n",
      "finished global work for group 3 and peer 5, acc test global :0.9256926952141058\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "# Global dictionary to store models for each group and peer\n",
    "global_objects = {}\n",
    "global_accuracies = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(float))))\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, group, peer):\n",
    "        self.group = group\n",
    "        self.peer = peer\n",
    "        self.counter = 0\n",
    "\n",
    "    def read_model_and_evaluate(self, mode='global'):\n",
    "        global global_accuracies\n",
    "        acc_train = -1.0\n",
    "        acc_test = -1.0\n",
    "        print('group', self.group, 'peer', self.peer, 'mode', mode)\n",
    "\n",
    "        model = globals()[f'model_group{self.group}_peer{self.peer}']\n",
    "        \n",
    "        if mode == 'global':\n",
    "            df_tot = globals()[f'train_df_group{group}'].copy()\n",
    "            self.df_tst = globals()[f'test_df_group{group}'].copy()\n",
    "        elif mode == 'local':\n",
    "            df_tot = globals()[f'train_df_group{group}_peer{peer}'].copy()\n",
    "            self.df_tst = globals()[f'test_df_group{group}_peer{peer}'].copy()\n",
    "        \n",
    "        print(df_tot.shape, self.df_tst.shape)\n",
    "\n",
    "        self.df_tst['generated_doc_id'] = self.df_tst['query'].apply(lambda x: self.generate_text_beams(x, model))\n",
    "        acc_test = self.df_tst.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / self.df_tst.shape[0]\n",
    "        \n",
    "        \n",
    "        global global_objects\n",
    "        global_objects[(self.group, self.peer)] = self.df_tst\n",
    "        \n",
    "        print(f'{mode} training set accuracy: ', acc_train)\n",
    "        print(f'{mode} test set accuracy: ', acc_test)\n",
    "        return acc_train, acc_test\n",
    "\n",
    "    def generate_text_beams(self, query, model):\n",
    "        self.counter += 1\n",
    "        if self.counter % 1000 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        input_ids = tokenizer.encode(query, return_tensors='pt')\n",
    "        output = model.generate(input_ids, do_sample=False, max_length=20,\n",
    "                                num_beams=5, num_return_sequences=5)\n",
    "        return [tokenizer.decode(i, skip_special_tokens=True) for i in output]\n",
    "\n",
    "    def thread_function(self):\n",
    "        global global_accuracies_20samples\n",
    "        acc_train_global, acc_test_global = self.read_model_and_evaluate('global')\n",
    "        global_accuracies[self.group][self.peer]['train']['global'] = acc_train_global\n",
    "        global_accuracies[self.group][self.peer]['test']['global'] = acc_test_global\n",
    "        print(f'finished global work for group {self.group} and peer {self.peer}, acc test global :{acc_test_global}')\n",
    "\n",
    "def evaluate_in_thread(group, peer):\n",
    "    try:\n",
    "        evaluator = ModelEvaluator(group, peer)\n",
    "        evaluator.thread_function()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in thread for group {group} and peer {peer}: {e}\")\n",
    "\n",
    "    \n",
    "# Start threads directly in the main script body\n",
    "threads = []\n",
    "for group in groups:\n",
    "    for peer in peers:\n",
    "        p = int(peer) - 8089\n",
    "        thread = threading.Thread(target=evaluate_in_thread, args=(group, p,))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    # Wait for all threads to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "941f5575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'1': defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
       "                         {1: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9277818270165209})}),\n",
       "                          7: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9149659863945578})}),\n",
       "                          10: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.8914601554907677})}),\n",
       "                          3: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9186710398445093})}),\n",
       "                          8: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9234086491739553})}),\n",
       "                          9: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.924198250728863})}),\n",
       "                          5: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9241375121477162})}),\n",
       "                          6: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9289358600583091})}),\n",
       "                          4: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9233479105928085})}),\n",
       "                          2: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9275388726919339})})}),\n",
       "             '2': defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
       "                         {8: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9123434704830053})}),\n",
       "                          1: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9195568122799931})}),\n",
       "                          9: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9211148941081424})}),\n",
       "                          5: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9140746725142824})}),\n",
       "                          10: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9186912112643546})}),\n",
       "                          3: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9158058745455595})}),\n",
       "                          6: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9220382018581569})}),\n",
       "                          2: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9147671533267933})}),\n",
       "                          4: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9133821917017716})}),\n",
       "                          7: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9242310577644411})})}),\n",
       "             '3': defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
       "                         {3: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9198152812762385})}),\n",
       "                          1: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9313901883171405})}),\n",
       "                          9: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9316900563751949})}),\n",
       "                          7: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9186757826556315})}),\n",
       "                          10: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9274919035624325})}),\n",
       "                          6: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9314501619287513})}),\n",
       "                          2: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9292911119107593})}),\n",
       "                          4: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9319899244332494})}),\n",
       "                          8: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9334292911119108})}),\n",
       "                          5: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9256926952141058})})})})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b620405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('global_accuracies_5beams.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2bcdeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {1: {'train': {'global': -1.0}, 'test': {'global': 0.9277818270165209}},\n",
       "  7: {'train': {'global': -1.0}, 'test': {'global': 0.9149659863945578}},\n",
       "  10: {'train': {'global': -1.0}, 'test': {'global': 0.8914601554907677}},\n",
       "  3: {'train': {'global': -1.0}, 'test': {'global': 0.9186710398445093}},\n",
       "  8: {'train': {'global': -1.0}, 'test': {'global': 0.9234086491739553}},\n",
       "  9: {'train': {'global': -1.0}, 'test': {'global': 0.924198250728863}},\n",
       "  5: {'train': {'global': -1.0}, 'test': {'global': 0.9241375121477162}},\n",
       "  6: {'train': {'global': -1.0}, 'test': {'global': 0.9289358600583091}},\n",
       "  4: {'train': {'global': -1.0}, 'test': {'global': 0.9233479105928085}},\n",
       "  2: {'train': {'global': -1.0}, 'test': {'global': 0.9275388726919339}}},\n",
       " '2': {8: {'train': {'global': -1.0}, 'test': {'global': 0.9123434704830053}},\n",
       "  1: {'train': {'global': -1.0}, 'test': {'global': 0.9195568122799931}},\n",
       "  9: {'train': {'global': -1.0}, 'test': {'global': 0.9211148941081424}},\n",
       "  5: {'train': {'global': -1.0}, 'test': {'global': 0.9140746725142824}},\n",
       "  10: {'train': {'global': -1.0}, 'test': {'global': 0.9186912112643546}},\n",
       "  3: {'train': {'global': -1.0}, 'test': {'global': 0.9158058745455595}},\n",
       "  6: {'train': {'global': -1.0}, 'test': {'global': 0.9220382018581569}},\n",
       "  2: {'train': {'global': -1.0}, 'test': {'global': 0.9147671533267933}},\n",
       "  4: {'train': {'global': -1.0}, 'test': {'global': 0.9133821917017716}},\n",
       "  7: {'train': {'global': -1.0}, 'test': {'global': 0.9242310577644411}}},\n",
       " '3': {3: {'train': {'global': -1.0}, 'test': {'global': 0.9198152812762385}},\n",
       "  1: {'train': {'global': -1.0}, 'test': {'global': 0.9313901883171405}},\n",
       "  9: {'train': {'global': -1.0}, 'test': {'global': 0.9316900563751949}},\n",
       "  7: {'train': {'global': -1.0}, 'test': {'global': 0.9186757826556315}},\n",
       "  10: {'train': {'global': -1.0}, 'test': {'global': 0.9274919035624325}},\n",
       "  6: {'train': {'global': -1.0}, 'test': {'global': 0.9314501619287513}},\n",
       "  2: {'train': {'global': -1.0}, 'test': {'global': 0.9292911119107593}},\n",
       "  4: {'train': {'global': -1.0}, 'test': {'global': 0.9319899244332494}},\n",
       "  8: {'train': {'global': -1.0}, 'test': {'global': 0.9334292911119108}},\n",
       "  5: {'train': {'global': -1.0}, 'test': {'global': 0.9256926952141058}}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('global_accuracies_5beams.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a57d2",
   "metadata": {},
   "source": [
    "# Sampling random models and aggregating their suggestions - 5 beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_groups_list = []\n",
    "three_groups_list = []\n",
    "for group in groups:\n",
    "    for i, peer in enumerate(peers):\n",
    "        exec(f'three_groups_list.append(model_group{group}_peer{int(peer)-8089})')\n",
    "        if int(group)<3:\n",
    "            exec(f'two_groups_list.append(model_group{group}_peer{int(peer)-8089})')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_list, train_df, test_df, tokenizer):\n",
    "        self.model_list = model_list\n",
    "        self.train_df = train_df.copy()\n",
    "        self.test_df = test_df.copy()\n",
    "        \n",
    "        print ('train set size:', self.train_df.shape[0])\n",
    "        print ('test set size:', self.test_df.shape[0])\n",
    "    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.counter = 0\n",
    "\n",
    "    def generate_text_beams(self, query):\n",
    "        self.counter += 1\n",
    "        if self.counter % 2000 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        results = defaultdict(float)\n",
    "        sampled_models = random.sample(self.model_list, 5)\n",
    "\n",
    "        for model in sampled_models:\n",
    "            # Ensure query is properly encoded\n",
    "            input_ids = self.tokenizer.encode(query, return_tensors='pt')\n",
    "            output = model.generate(input_ids, do_sample=False, return_dict_in_generate=True, output_scores=True,\n",
    "                                    num_beams=5, num_return_sequences=5)\n",
    "\n",
    "#             beam_scores = output.sequences_scores\n",
    "#             probabilities = softmax(beam_scores, dim=0).tolist()\n",
    "            model_res = [self.tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output.sequences]\n",
    "            model_res = list(set(model_res))\n",
    "        \n",
    "            for res in model_res:\n",
    "                results[res] += 1\n",
    "\n",
    "        return self.top_5_generated_texts(results)\n",
    "\n",
    "    def top_5_generated_texts(self, input_dict):\n",
    "        sorted_responses = sorted(input_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [response[0] for response in sorted_responses[:5]]\n",
    "\n",
    "    def evaluate_accuracy(self):\n",
    "        self.train_df['generated_doc_id'] = self.train_df['query'].apply(self.generate_text_beams)\n",
    "        self.test_df['generated_doc_id'] = self.test_df['query'].apply(self.generate_text_beams)\n",
    "\n",
    "        acc_train = self.calculate_accuracy(self.train_df)\n",
    "        acc_test = self.calculate_accuracy(self.test_df)\n",
    "\n",
    "        return acc_train, acc_test\n",
    "\n",
    "    def calculate_accuracy(self, df):\n",
    "        return df.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / df.shape[0]\n",
    "\n",
    "# Assuming you have predefined dictionaries/lists for models and datasets, such as:\n",
    "# two_groups_list = [...]\n",
    "# three_groups_list = [...]\n",
    "# train_df_group1 = ...\n",
    "# test_df_group1 = ...\n",
    "# ... and so on for other groups\n",
    "# And a tokenizer instance\n",
    "\n",
    "def run_evaluation(group_nbr, model_list_nbr, tokenizer):\n",
    "    if model_list_nbr == 'two':\n",
    "        model_list = two_groups_list\n",
    "    elif model_list_nbr == 'three':\n",
    "        model_list = three_groups_list\n",
    "\n",
    "    train_df = globals()[f'train_df_group{group_nbr}']\n",
    "    test_df = globals()[f'test_df_group{group_nbr}']\n",
    "    \n",
    "    manager = ModelManager(model_list, train_df, test_df, tokenizer)\n",
    "    \n",
    "    key = (group_nbr, model_list_nbr)\n",
    "    model_managers[key] = manager\n",
    "    \n",
    "    \n",
    "    acc_train, acc_test = manager.evaluate_accuracy()\n",
    "    global_accuracies[key] = {'acc_train': acc_train, 'acc_test': acc_test}\n",
    "\n",
    "    print(f\"Group: {group_nbr}, Model List: {model_list_nbr}, Train Acc: {acc_train}, Test Acc: {acc_test}\")\n",
    "\n",
    "    \n",
    "# Global collection to store ModelManager instances\n",
    "model_managers = {}\n",
    "global_accuracies = {}\n",
    "    \n",
    "# Threading\n",
    "threads = []\n",
    "\n",
    "for group_nbr in range(1, 4):\n",
    "    for model_list_nbr in ['two', 'three']:\n",
    "        if group_nbr == 3 and model_list_nbr == 'two':\n",
    "            continue\n",
    "        thread = threading.Thread(target=run_evaluation, args=(group_nbr, model_list_nbr, tokenizer))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('global_accuracies_samplingmodels_5beams.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d98e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('global_accuracies_samplingmodels_5beams.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211fca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('global_accuracies_samplingmodels_5beams.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a788bb",
   "metadata": {},
   "source": [
    "# Sampling random models and aggregating their suggestions - 1 beam - main suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86caf104",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_groups_list = []\n",
    "three_groups_list = []\n",
    "for group in groups:\n",
    "    for peer in peers:\n",
    "        exec(f'three_groups_list.append(model_group{group}_peer{int(peer)-8089})')\n",
    "        if int(group)<3:\n",
    "            exec(f'two_groups_list.append(model_group{group}_peer{int(peer)-8089})')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "918ee503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size:train set size: 16432\n",
      "test set size: 16464\n",
      " 16432\n",
      "test set size: 16464\n",
      "train set size: 17311\n",
      "test set size: 17329\n",
      "train set size: 17311\n",
      "test set size: 17329\n",
      "train set size: 16647\n",
      "test set size: 16674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_list, train_df, test_df, tokenizer):\n",
    "        self.model_list = model_list\n",
    "        self.train_df = train_df.copy()\n",
    "        self.test_df = test_df.copy()\n",
    "        \n",
    "        print ('train set size:', self.train_df.shape[0])\n",
    "        print ('test set size:', self.test_df.shape[0])\n",
    "    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.counter = 0\n",
    "\n",
    "    def generate_text_beams(self, query):\n",
    "        self.counter += 1\n",
    "        if self.counter % 1000 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        results = defaultdict(float)\n",
    "        sampled_models = random.sample(self.model_list, 5)\n",
    "\n",
    "        for model in sampled_models:\n",
    "            # Ensure query is properly encoded\n",
    "            input_ids = self.tokenizer.encode(query, return_tensors='pt')\n",
    "            output = model.generate(input_ids, do_sample=False, return_dict_in_generate=True, output_scores=True,\n",
    "                                    num_beams=5, num_return_sequences=5)\n",
    "\n",
    "#             beam_scores = output.sequences_scores\n",
    "#             probabilities = softmax(beam_scores, dim=0).tolist()\n",
    "            model_res = [self.tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output.sequences]\n",
    "            model_res = list(set(model_res))\n",
    "            for res in model_res:\n",
    "                results[res] += 1\n",
    "\n",
    "        return self.top_5_generated_texts(results)\n",
    "\n",
    "    def top_5_generated_texts(self, input_dict):\n",
    "        sorted_responses = sorted(input_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [response[0] for response in sorted_responses[:1]]\n",
    "\n",
    "    def evaluate_accuracy(self):\n",
    "        self.train_df['generated_doc_id'] = self.train_df['query'].apply(self.generate_text_beams)\n",
    "        self.test_df['generated_doc_id'] = self.test_df['query'].apply(self.generate_text_beams)\n",
    "\n",
    "        acc_train = self.calculate_accuracy(self.train_df)\n",
    "        acc_test = self.calculate_accuracy(self.test_df)\n",
    "\n",
    "        return acc_train, acc_test\n",
    "\n",
    "    def calculate_accuracy(self, df):\n",
    "        return df.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / df.shape[0]\n",
    "\n",
    "# Assuming you have predefined dictionaries/lists for models and datasets, such as:\n",
    "# two_groups_list = [...]\n",
    "# three_groups_list = [...]\n",
    "# train_df_group1 = ...\n",
    "# test_df_group1 = ...\n",
    "# ... and so on for other groups\n",
    "# And a tokenizer instance\n",
    "\n",
    "def run_evaluation(group_nbr, model_list_nbr, tokenizer):\n",
    "    if model_list_nbr == 'two':\n",
    "        model_list = two_groups_list\n",
    "    elif model_list_nbr == 'three':\n",
    "        model_list = three_groups_list\n",
    "\n",
    "    train_df = globals()[f'train_df_group{group_nbr}']\n",
    "    test_df = globals()[f'test_df_group{group_nbr}']\n",
    "\n",
    "    manager = ModelManager(model_list, train_df, test_df, tokenizer)\n",
    "    \n",
    "    key = (group_nbr, model_list_nbr)\n",
    "    model_managers[key] = manager\n",
    "    \n",
    "    \n",
    "    acc_train, acc_test = manager.evaluate_accuracy()\n",
    "    global_accuracies[key] = {'acc_train': acc_train, 'acc_test': acc_test}\n",
    "\n",
    "    print(f\"Group: {group_nbr}, Model List: {model_list_nbr}, Train Acc: {acc_train}, Test Acc: {acc_test}\")\n",
    "\n",
    "    \n",
    "# Global collection to store ModelManager instances\n",
    "model_managers = {}\n",
    "global_accuracies = {}\n",
    "    \n",
    "# Threading\n",
    "threads = []\n",
    "\n",
    "for group_nbr in range(1, 4):\n",
    "    for model_list_nbr in ['two', 'three']:\n",
    "        if group_nbr == 3 and model_list_nbr == 'two':\n",
    "            continue\n",
    "        thread = threading.Thread(target=run_evaluation, args=(group_nbr, model_list_nbr, tokenizer))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f86061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('global_accuracies_samplingmodels.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c75a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc3e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('global_accuracies_samplingmodels.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d88a2",
   "metadata": {},
   "source": [
    "# Sampling random models and aggregating their suggestions - 5 beams, with probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f25364",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_groups_list = []\n",
    "three_groups_list = []\n",
    "for group in groups:\n",
    "    for peer in peers:\n",
    "        exec(f'three_groups_list.append(model_group{group}_peer{int(peer)-8089})')\n",
    "        if int(group)<3:\n",
    "            exec(f'two_groups_list.append(model_group{group}_peer{int(peer)-8089})')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5eaaf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Exception in threading.excepthook:\n",
      "Exception ignored in thread started by: <bound method Thread._bootstrap of <Thread(Thread-13 (run_evaluation), stopped 18488766464)>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/petruneague/anaconda3/lib/python3.11/threading.py\", line 995, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/Users/petruneague/anaconda3/lib/python3.11/threading.py\", line 1040, in _bootstrap_inner\n",
      "    self._invoke_excepthook(self)\n",
      "  File \"/Users/petruneague/anaconda3/lib/python3.11/threading.py\", line 1352, in invoke_excepthook\n",
      "    local_print(\"Exception in threading.excepthook:\",\n",
      "  File \"/Users/petruneague/anaconda3/lib/python3.11/site-packages/ipykernel/iostream.py\", line 559, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/Users/petruneague/anaconda3/lib/python3.11/site-packages/ipykernel/iostream.py\", line 251, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/Users/petruneague/anaconda3/lib/python3.11/site-packages/zmq/sugar/socket.py\", line 618, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 740, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 781, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 137, in zmq.backend.cython.socket._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n",
      "Exception ignored in sys.unraisablehook: <built-in function unraisablehook>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/petruneague/anaconda3/lib/python3.11/site-packages/ipykernel/iostream.py\", line 559, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/Users/petruneague/anaconda3/lib/python3.11/site-packages/ipykernel/iostream.py\", line 251, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/Users/petruneague/anaconda3/lib/python3.11/site-packages/zmq/sugar/socket.py\", line 618, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 740, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 781, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 137, in zmq.backend.cython.socket._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train set size: 16432\n",
      "test set size: 16464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petruneague/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL INDEXES [4, 1, 12, 19, 16]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D176337': 1.3625423908233643, 'D3254647': 0.09982369095087051, 'D3134428': 0.09474887698888779, 'D3258344': 0.08658268302679062, 'D3147911': 0.07561706006526947, 'D1763373': 0.09104155004024506, 'D1763372': 0.06819207221269608, 'D1763187': 0.061951905488967896, 'D1763106': 0.059499699622392654, 'D2585416': 0.22938866913318634, 'D2689316': 0.22170831263065338, 'D2585336': 0.2109907567501068, 'D2541782': 0.5361056625843048, 'D3175149': 0.14857006072998047, 'D1835464': 0.2236683964729309, 'D1850707': 0.15044063329696655, 'D1957508': 0.5873086750507355, 'D1959455': 0.13026893138885498, 'D1953908': 0.16385704278945923, 'D354714': 0.14231367409229279, 'D1956508': 0.1314542591571808, 'D1955508': 0.12392492592334747})\n",
      "TOP 5 ['D176337', 'D1957508', 'D2541782', 'D2585416', 'D1835464']\n",
      "D176337\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D176337': 2.0, 'D3254647': 1.0, 'D3134428': 1.0, 'D3258344': 1.0, 'D3147911': 1.0, 'D1763373': 1.0, 'D1763372': 1.0, 'D1763187': 1.0, 'D1763106': 1.0, 'D2585416': 1.0, 'D2689316': 1.0, 'D2585336': 1.0, 'D2541782': 2.0, 'D3175149': 1.0, 'D1835464': 1.0, 'D1850707': 1.0, 'D1957508': 2.0, 'D1959455': 1.0, 'D1953908': 1.0, 'D354714': 1.0, 'D1956508': 1.0, 'D1955508': 1.0})\n",
      "TOP 5 ['D176337', 'D2541782', 'D1957508', 'D3254647', 'D3134428']\n",
      "D176337\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "MODEL INDEXES [12, 15, 4, 19, 0]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D2003299': 0.22783857583999634, 'D2623392': 0.21247904002666473, 'D2005211': 0.20418772101402283, 'D2005412': 0.1885424703359604, 'D2003740': 0.1669522374868393, 'D2669483': 0.25750401616096497, 'D263172': 0.2070520967245102, 'D2670382': 0.19563636183738708, 'D2671483': 0.17501108348369598, 'D2966300': 0.1647963970899582, 'D2267233': 1.4507505893707275, 'D2268233': 0.07845962047576904, 'D2265233': 0.0447520911693573, 'D2222067': 0.03298977017402649, 'D2222061': 0.032704442739486694, 'D520279': 0.3557761311531067, 'D2160438': 0.18148335814476013, 'D2169266': 0.16104015707969666, 'D2169893': 0.15230853855609894, 'D2160238': 0.1493917554616928, 'D2299233': 0.119140625, 'D2263233': 0.11319218575954437, 'D2273233': 0.06496543437242508, 'D2263143': 0.06304529309272766})\n",
      "TOP 5 ['D2267233', 'D520279', 'D2669483', 'D2003299', 'D2623392']\n",
      "D2267233\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D2003299': 1.0, 'D2623392': 1.0, 'D2005211': 1.0, 'D2005412': 1.0, 'D2003740': 1.0, 'D2669483': 1.0, 'D263172': 1.0, 'D2670382': 1.0, 'D2671483': 1.0, 'D2966300': 1.0, 'D2267233': 2.0, 'D2268233': 1.0, 'D2265233': 1.0, 'D2222067': 1.0, 'D2222061': 1.0, 'D520279': 1.0, 'D2160438': 1.0, 'D2169266': 1.0, 'D2169893': 1.0, 'D2160238': 1.0, 'D2299233': 1.0, 'D2263233': 1.0, 'D2273233': 1.0, 'D2263143': 1.0})\n",
      "TOP 5 ['D2267233', 'D2003299', 'D2623392', 'D2005211', 'D2005412']\n",
      "D2267233\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "MODEL INDEXES [15, 18, 7, 19, 3]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D3104303': 0.2405722439289093, 'D3267923': 0.20585085451602936, 'D3267074': 0.20108814537525177, 'D2862647': 0.1885770708322525, 'D3242728': 0.1639116406440735, 'D3203425': 0.22563068568706512, 'D36841': 0.21433602273464203, 'D3282780': 0.2123841643333435, 'D3382772': 0.20053990185260773, 'D1633842': 0.14710921049118042, 'D1296311': 1.5073226690292358, 'D1296311  D1296311': 0.10841112583875656, 'D1296311  D1296311  D1296311': 0.06282099336385727, 'D1296311  D1296311 www D12963': 0.04272865131497383, 'D1296311  D1296311 www D1296311': 0.04242682456970215, 'D678588': 0.26261627674102783, 'D3138587': 0.23177413642406464, 'D675970': 0.1823391616344452, 'D678468': 0.16585364937782288, 'D675540': 0.15741682052612305, 'D1296311 ( D1296311': 0.13902608677744865, 'D1296311 The Educational D1296311': 0.04876643419265747, 'D1296311 (  D1296311': 0.04849737882614136})\n",
      "TOP 5 ['D1296311', 'D678588', 'D3104303', 'D3138587', 'D3203425']\n",
      "D1296311\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D3104303': 1.0, 'D3267923': 1.0, 'D3267074': 1.0, 'D2862647': 1.0, 'D3242728': 1.0, 'D3203425': 1.0, 'D36841': 1.0, 'D3282780': 1.0, 'D3382772': 1.0, 'D1633842': 1.0, 'D1296311': 2.0, 'D1296311  D1296311': 1.0, 'D1296311  D1296311  D1296311': 1.0, 'D1296311  D1296311 www D12963': 1.0, 'D1296311  D1296311 www D1296311': 1.0, 'D678588': 1.0, 'D3138587': 1.0, 'D675970': 1.0, 'D678468': 1.0, 'D675540': 1.0, 'D1296311 ( D1296311': 2.0, 'D1296311 The Educational D1296311': 1.0, 'D1296311 (  D1296311': 1.0})\n",
      "TOP 5 ['D1296311', 'D1296311 ( D1296311', 'D3104303', 'D3267923', 'D3267074']\n",
      "D1296311\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "MODEL INDEXES [8, 18, 19, 5, 2]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D432626': 0.795109897851944, 'D164843': 0.22548595070838928, 'D434863': 0.21827799081802368, 'D911856': 0.16305817663669586, 'D94286': 0.15975165367126465, 'D65196': 0.24121615290641785, 'D2670382': 0.2146000862121582, 'D2091632': 0.20105622708797455, 'D663129': 0.1892678588628769, 'D630286': 0.15385963022708893, 'D2669483': 0.36582228541374207, 'D2663483': 0.20813804864883423, 'D1840158': 0.1599045991897583, 'D1850158': 0.1447676569223404, 'D266303': 0.12136748433113098, 'D462209': 0.6773537695407867, 'D941608': 0.1570950448513031, 'D2622224': 0.1259363740682602, 'D462305': 0.09031646698713303, 'D468301': 0.15231561660766602, 'D484641': 0.12985920906066895, 'D486285': 0.10543985664844513})\n",
      "TOP 5 ['D432626', 'D462209', 'D2669483', 'D65196', 'D164843']\n",
      "D462209\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D432626': 3.0, 'D164843': 1.0, 'D434863': 1.0, 'D911856': 1.0, 'D94286': 1.0, 'D65196': 1.0, 'D2670382': 1.0, 'D2091632': 1.0, 'D663129': 1.0, 'D630286': 1.0, 'D2669483': 1.0, 'D2663483': 1.0, 'D1840158': 1.0, 'D1850158': 1.0, 'D266303': 1.0, 'D462209': 2.0, 'D941608': 1.0, 'D2622224': 1.0, 'D462305': 1.0, 'D468301': 1.0, 'D484641': 1.0, 'D486285': 1.0})\n",
      "TOP 5 ['D432626', 'D462209', 'D164843', 'D434863', 'D911856']\n",
      "D462209\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "MODEL INDEXES [4, 1, 12, 9, 10]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D3367699': 2.1542782932519913, 'D3367699 www www www D3367699': 0.12734147906303406, 'D3367699 www www www www D3367699': 0.12728925049304962, 'D3367699 www www www D33676': 0.2055797576904297, 'D3 367699': 0.047737929970026016, 'D3367649': 0.044899459928274155, 'D3367699.': 0.04378343001008034, 'D1282870': 0.647057294845581, 'D3563499': 0.19557081162929535, 'D1352869': 0.18801331520080566, 'D1356363': 0.18131855130195618, 'D2880728': 0.1653318703174591, 'D33367699': 0.0774122104048729, 'D3377699': 0.06368791311979294, 'D3367699 D': 0.05484404042363167, 'D3367699 D3': 0.053146325051784515, 'D2181919': 0.20198699831962585, 'D1354595': 0.14769843220710754, 'D1289659': 0.14430223405361176, 'D1284647': 0.1287204474210739})\n",
      "TOP 5 ['D3367699', 'D1282870', 'D3367699 www www www D33676', 'D2181919', 'D3563499']\n",
      "D3367699\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D3367699': 4.0, 'D3367699 www www www D3367699': 1.0, 'D3367699 www www www www D3367699': 1.0, 'D3367699 www www www D33676': 2.0, 'D3 367699': 1.0, 'D3367649': 1.0, 'D3367699.': 1.0, 'D1282870': 2.0, 'D3563499': 1.0, 'D1352869': 1.0, 'D1356363': 1.0, 'D2880728': 1.0, 'D33367699': 1.0, 'D3377699': 1.0, 'D3367699 D': 1.0, 'D3367699 D3': 1.0, 'D2181919': 1.0, 'D1354595': 1.0, 'D1289659': 1.0, 'D1284647': 1.0})\n",
      "TOP 5 ['D3367699', 'D3367699 www www www D33676', 'D1282870', 'D3367699 www www www D3367699', 'D3367699 www www www www D3367699']\n",
      "D3367699\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "MODEL INDEXES [9, 15, 2, 14, 4]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D1270873': 2.5287243127822876, 'D1271493': 0.05406073480844498, 'D1270273': 0.07990409806370735, 'D1270973': 0.04598104953765869, 'D1270773': 0.14030898734927177, 'D211020': 0.7566203773021698, 'D2130241': 0.24059826135635376, 'D2640584': 0.15201930701732635, 'D269396': 0.13537722826004028, 'D264058': 0.1314411461353302, 'D1270876': 0.03550202026963234, 'D1264508': 0.03165815770626068, 'D1730334': 0.22800298035144806, 'D272153': 0.143527552485466, 'D21186': 0.12370070070028305, 'D2350607': 0.08871243894100189, 'D1270874': 0.03429688885807991, 'D1270673': 0.028793537989258766, 'D1270873.': 0.020770125091075897})\n",
      "TOP 5 ['D1270873', 'D211020', 'D2130241', 'D1730334', 'D2640584']\n",
      "D1270873\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D1270873': 3.0, 'D1271493': 1.0, 'D1270273': 2.0, 'D1270973': 1.0, 'D1270773': 3.0, 'D211020': 2.0, 'D2130241': 1.0, 'D2640584': 1.0, 'D269396': 1.0, 'D264058': 1.0, 'D1270876': 1.0, 'D1264508': 1.0, 'D1730334': 1.0, 'D272153': 1.0, 'D21186': 1.0, 'D2350607': 1.0, 'D1270874': 1.0, 'D1270673': 1.0, 'D1270873.': 1.0})\n",
      "TOP 5 ['D1270873', 'D1270773', 'D1270273', 'D211020', 'D1271493']\n",
      "D1270873\n",
      "True\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL INDEXES [7, 15, 3, 8, 4]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D3339549': 2.5131624713540077, 'D3385373': 0.10721108317375183, 'D3339769': 0.3039279133081436, 'D842733': 0.08849169313907623, 'D1334916': 0.07284964621067047, 'D551156': 0.32521554827690125, 'D2558370': 0.18580549955368042, 'D1636847': 0.17590709030628204, 'D1626677': 0.1575108766555786, 'D2281196': 0.15556101500988007, 'D33395494': 0.2737727016210556, 'D33395492': 0.09835365414619446, 'D33395493': 0.0783371850848198, 'D33395496': 0.07264116406440735, 'D3489849': 0.11040704697370529, 'D3489033': 0.09697996079921722, 'D3489066': 0.09525743871927261, 'D3337294': 0.08860819786787033})\n",
      "TOP 5 ['D3339549', 'D551156', 'D3339769', 'D33395494', 'D2558370']\n",
      "D3339549\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D3339549': 5.0, 'D3385373': 1.0, 'D3339769': 3.0, 'D842733': 1.0, 'D1334916': 1.0, 'D551156': 1.0, 'D2558370': 1.0, 'D1636847': 1.0, 'D1626677': 1.0, 'D2281196': 1.0, 'D33395494': 2.0, 'D33395492': 1.0, 'D33395493': 1.0, 'D33395496': 1.0, 'D3489849': 1.0, 'D3489033': 1.0, 'D3489066': 1.0, 'D3337294': 1.0})\n",
      "TOP 5 ['D3339549', 'D3339769', 'D33395494', 'D3385373', 'D842733']\n",
      "D3339549\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "MODEL INDEXES [15, 17, 3, 7, 19]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D250739': 0.23561057448387146, 'D250556': 0.22672313451766968, 'D250759': 0.1955333650112152, 'D253058': 0.18603132665157318, 'D2515365': 0.15610167384147644, 'D1512360': 0.22818246483802795, 'D2572027': 0.22064441442489624, 'D1570118': 0.2047276794910431, 'D1570360': 0.17719368636608124, 'D05904': 0.16925176978111267, 'D1447805': 1.739802848547697, 'D1447905': 0.09384148940443993, 'D1447805 D144': 0.032421719282865524, 'D1441799': 0.03191523253917694, 'D1224441': 0.03007981926202774, 'D740568': 0.04156387597322464, 'D945319': 0.030375050380825996, 'D2555849': 0.234500914812088, 'D2554954': 0.21304473280906677, 'D405752': 0.20126725733280182, 'D433578': 0.17678911983966827, 'D453068': 0.17439788579940796})\n",
      "TOP 5 ['D1447805', 'D250739', 'D2555849', 'D1512360', 'D250556']\n",
      "D1447805\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D250739': 1.0, 'D250556': 1.0, 'D250759': 1.0, 'D253058': 1.0, 'D2515365': 1.0, 'D1512360': 1.0, 'D2572027': 1.0, 'D1570118': 1.0, 'D1570360': 1.0, 'D05904': 1.0, 'D1447805': 3.0, 'D1447905': 2.0, 'D1447805 D144': 1.0, 'D1441799': 1.0, 'D1224441': 1.0, 'D740568': 1.0, 'D945319': 1.0, 'D2555849': 1.0, 'D2554954': 1.0, 'D405752': 1.0, 'D433578': 1.0, 'D453068': 1.0})\n",
      "TOP 5 ['D1447805', 'D1447905', 'D250739', 'D250556', 'D250759']\n",
      "D1447805\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "MODEL INDEXES [6, 4, 0, 15, 14]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D1899613': 0.7354225069284439, 'D2566158': 0.20769940316677094, 'D1791071': 0.11508146673440933, 'D1542662': 0.09759237617254257, 'D2613962': 0.09583491832017899, 'D1898352': 1.0243346095085144, 'D1898951': 0.21612834930419922, 'D1894259': 0.09586860239505768, 'D1898952': 0.09516042470932007, 'D1898353': 0.12195292115211487, 'D1898351': 0.12005773186683655, 'D1898342': 0.0748666450381279, 'D143768': 0.2157050520181656, 'D143767': 0.20760557055473328, 'D146758': 0.19375552237033844, 'D162564': 0.19356459379196167, 'D148549': 0.1893693059682846, 'D1512360': 0.2390965223312378, 'D1267999': 0.20788483321666718, 'D143022': 0.18619292974472046, 'D123947': 0.185061514377594, 'D1435370': 0.1817641705274582})\n",
      "TOP 5 ['D1898352', 'D1899613', 'D1512360', 'D1898951', 'D143768']\n",
      "D1898352\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D1899613': 3.0, 'D2566158': 1.0, 'D1791071': 1.0, 'D1542662': 1.0, 'D2613962': 1.0, 'D1898352': 2.0, 'D1898951': 1.0, 'D1894259': 1.0, 'D1898952': 1.0, 'D1898353': 1.0, 'D1898351': 1.0, 'D1898342': 1.0, 'D143768': 1.0, 'D143767': 1.0, 'D146758': 1.0, 'D162564': 1.0, 'D148549': 1.0, 'D1512360': 1.0, 'D1267999': 1.0, 'D143022': 1.0, 'D123947': 1.0, 'D1435370': 1.0})\n",
      "TOP 5 ['D1899613', 'D1898352', 'D2566158', 'D1791071', 'D1542662']\n",
      "D1898352\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "MODEL INDEXES [6, 2, 0, 14, 19]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D986008': 2.518989847972989, 'D986007': 0.1666324995458126, 'D986006': 0.13571611605584621, 'D989008': 0.08927152678370476, 'D988008': 0.08938991650938988, 'D1597991': 0.21486467123031616, 'D678588': 0.203107550740242, 'D1553318': 0.19998124241828918, 'D656544': 0.19758106768131256, 'D27632': 0.18446552753448486, 'D3251806': 0.23458071053028107, 'D2671367': 0.2141641080379486, 'D604327': 0.19574420154094696, 'D2677687': 0.1845141500234604, 'D2673406': 0.1709968000650406})\n",
      "TOP 5 ['D986008', 'D3251806', 'D1597991', 'D2671367', 'D678588']\n",
      "D986008\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D986008': 5.0, 'D986007': 3.0, 'D986006': 3.0, 'D989008': 2.0, 'D988008': 2.0, 'D1597991': 1.0, 'D678588': 1.0, 'D1553318': 1.0, 'D656544': 1.0, 'D27632': 1.0, 'D3251806': 1.0, 'D2671367': 1.0, 'D604327': 1.0, 'D2677687': 1.0, 'D2673406': 1.0})\n",
      "TOP 5 ['D986008', 'D986007', 'D986006', 'D989008', 'D988008']\n",
      "D986008\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "MODEL INDEXES [6, 14, 10, 18, 0]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D453236': 1.132745623588562, 'D429317': 0.14541010558605194, 'D432626': 0.12724097073078156, 'D2911601': 0.09397408366203308, 'D3445036': 0.06653619557619095, 'D1833308': 0.9903528690338135, 'D1738810': 0.23764578998088837, 'D1942520': 0.20682834088802338, 'D2023832': 0.12713953852653503, 'D2110660': 0.11603181809186935, 'D1595187': 0.21852260828018188, 'D1413895': 0.21095037460327148, 'D1435600': 0.15445064008235931, 'D1414214': 0.1539647877216339, 'D1733308': 0.1784498393535614, 'D1517292': 0.13788869976997375, 'D2128083': 0.13753536343574524, 'D1833309': 0.1302393674850464, 'D4532363': 0.14780782163143158, 'D4633': 0.10496197640895844, 'D45323634': 0.09117844700813293, 'D4532362': 0.0901448130607605})\n",
      "TOP 5 ['D453236', 'D1833308', 'D1738810', 'D1595187', 'D1413895']\n",
      "D453236\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D453236': 2.0, 'D429317': 1.0, 'D432626': 1.0, 'D2911601': 1.0, 'D3445036': 1.0, 'D1833308': 3.0, 'D1738810': 1.0, 'D1942520': 1.0, 'D2023832': 1.0, 'D2110660': 1.0, 'D1595187': 1.0, 'D1413895': 1.0, 'D1435600': 1.0, 'D1414214': 1.0, 'D1733308': 1.0, 'D1517292': 1.0, 'D2128083': 1.0, 'D1833309': 1.0, 'D4532363': 1.0, 'D4633': 1.0, 'D45323634': 1.0, 'D4532362': 1.0})\n",
      "TOP 5 ['D1833308', 'D453236', 'D429317', 'D432626', 'D2911601']\n",
      "D453236\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "MODEL INDEXES [3, 7, 14, 1, 9]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D3367699': 2.921747673302889, 'D 3367699': 0.08797762542963028, 'D4367699': 0.08747293800115585, 'D3,367699': 0.07717353105545044, 'D2367699': 0.07302132248878479, 'D3769948': 0.05932551622390747, 'D3769998': 0.05253242328763008, 'D43676': 0.04515326768159866, 'D184405': 0.3283788859844208, 'D2091632': 0.23693063855171204, 'D1840158': 0.1589561253786087, 'D194860': 0.14183150231838226, 'D2094197': 0.1339028924703598, 'D3367699 — Tel D3367699': 0.10732551664113998, 'D3367699 — 767699': 0.06584584712982178, 'D3367699 – 967699': 0.06569556146860123, 'D3367699 — 7699': 0.05237535387277603, 'D3367699 D': 0.08438429236412048, 'D3367699 D D3367699': 0.08013363927602768, 'D3377699': 0.07114070653915405, 'D3367699 D3': 0.0686950609087944})\n",
      "TOP 5 ['D3367699', 'D184405', 'D2091632', 'D1840158', 'D194860']\n",
      "D3367699\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D3367699': 5.0, 'D 3367699': 1.0, 'D4367699': 1.0, 'D3,367699': 1.0, 'D2367699': 1.0, 'D3769948': 1.0, 'D3769998': 1.0, 'D43676': 1.0, 'D184405': 1.0, 'D2091632': 1.0, 'D1840158': 1.0, 'D194860': 1.0, 'D2094197': 1.0, 'D3367699 — Tel D3367699': 1.0, 'D3367699 — 767699': 1.0, 'D3367699 – 967699': 1.0, 'D3367699 — 7699': 1.0, 'D3367699 D': 1.0, 'D3367699 D D3367699': 1.0, 'D3377699': 1.0, 'D3367699 D3': 1.0})\n",
      "TOP 5 ['D3367699', 'D 3367699', 'D4367699', 'D3,367699', 'D2367699']\n",
      "D3367699\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "MODEL INDEXES [12, 4, 0, 8, 11]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D162564': 0.29866936802864075, 'D164384': 0.2003372311592102, 'D1623750': 0.1813511997461319, 'D162231': 0.17201602458953857, 'D1623370': 0.1476260870695114, 'D3015803': 1.5122548639774323, 'D323015803': 0.10175001621246338, 'D383240': 0.21117838472127914, 'D3415803': 0.07208637148141861, 'D333830': 0.06774576008319855, 'D1285454': 0.15814290940761566, 'D3420175': 0.14531183242797852, 'D123819': 0.12414853274822235, 'D896563': 0.1900463104248047, 'D308817': 0.1707659363746643, 'D799595': 0.1282283514738083, 'D3087194': 0.11834071576595306, 'D1324402': 0.22707073390483856, 'D3139841': 0.2088523954153061, 'D3229520': 0.19639520347118378, 'D2938385': 0.18448813259601593, 'D3139842': 0.18319347500801086})\n",
      "TOP 5 ['D3015803', 'D162564', 'D1324402', 'D383240', 'D3139841']\n",
      "D3015803\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D162564': 1.0, 'D164384': 1.0, 'D1623750': 1.0, 'D162231': 1.0, 'D1623370': 1.0, 'D3015803': 3.0, 'D323015803': 1.0, 'D383240': 2.0, 'D3415803': 1.0, 'D333830': 1.0, 'D1285454': 1.0, 'D3420175': 1.0, 'D123819': 1.0, 'D896563': 1.0, 'D308817': 1.0, 'D799595': 1.0, 'D3087194': 1.0, 'D1324402': 1.0, 'D3139841': 1.0, 'D3229520': 1.0, 'D2938385': 1.0, 'D3139842': 1.0})\n",
      "TOP 5 ['D3015803', 'D383240', 'D162564', 'D164384', 'D1623750']\n",
      "D3015803\n",
      "True\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL INDEXES [0, 3, 8, 1, 12]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D576017': 1.978517770767212, 'D578420': 0.17356082797050476, 'D578417': 0.13349057734012604, 'D578430': 0.1262706220149994, 'D578401': 0.12428362667560577, 'D603621': 0.12882114946842194, 'D5760172': 0.11842114478349686, 'D578017': 0.11146648228168488, 'D1676047': 0.10491794347763062, 'D583161': 0.09902426600456238, 'D562659': 0.08927366882562637, 'D576111': 0.07820598781108856, 'D576117': 0.07782918959856033, 'D576098': 0.3114544749259949, 'D596098': 0.13545458018779755, 'D635807': 0.10688721388578415, 'D63543': 0.10212058573961258, 'D1803897': 0.2638486921787262, 'D1805775': 0.19984786212444305, 'D1803740': 0.1847630888223648, 'D1113889': 0.17927545309066772, 'D1805520': 0.17226482927799225})\n",
      "TOP 5 ['D576017', 'D576098', 'D1803897', 'D1805775', 'D1803740']\n",
      "D576017\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D576017': 4.0, 'D578420': 1.0, 'D578417': 1.0, 'D578430': 1.0, 'D578401': 1.0, 'D603621': 1.0, 'D5760172': 1.0, 'D578017': 1.0, 'D1676047': 1.0, 'D583161': 1.0, 'D562659': 1.0, 'D576111': 1.0, 'D576117': 1.0, 'D576098': 1.0, 'D596098': 1.0, 'D635807': 1.0, 'D63543': 1.0, 'D1803897': 1.0, 'D1805775': 1.0, 'D1803740': 1.0, 'D1113889': 1.0, 'D1805520': 1.0})\n",
      "TOP 5 ['D576017', 'D578420', 'D578417', 'D578430', 'D578401']\n",
      "D576017\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "MODEL INDEXES [3, 5, 18, 17, 6]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D432626': 0.5237181037664413, 'D2472276': 0.916157677769661, 'D2536401': 0.1599084436893463, 'D2533657': 0.12792576849460602, 'D2524757': 0.12139420956373215, 'D236010': 0.3863779306411743, 'D2452276': 0.1516689658164978, 'D333930': 0.12263672798871994, 'D3386116': 0.11561377346515656, 'D1399651': 0.20957691967487335, 'D1835464': 0.20588688552379608, 'D1850455': 0.20368421077728271, 'D1438473': 0.20175637304782867, 'D143164': 0.17909562587738037, 'D103164': 0.26673656702041626, 'D1036612': 0.22904284298419952, 'D1176370': 0.18645897507667542, 'D1176864': 0.15953277051448822, 'D179739': 0.15822885930538177, 'D2368649': 0.20692406594753265, 'D22916': 0.16767418384552002})\n",
      "TOP 5 ['D2472276', 'D432626', 'D236010', 'D103164', 'D1036612']\n",
      "D2472276\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D432626': 2.0, 'D2472276': 3.0, 'D2536401': 1.0, 'D2533657': 1.0, 'D2524757': 1.0, 'D236010': 2.0, 'D2452276': 1.0, 'D333930': 1.0, 'D3386116': 1.0, 'D1399651': 1.0, 'D1835464': 1.0, 'D1850455': 1.0, 'D1438473': 1.0, 'D143164': 1.0, 'D103164': 1.0, 'D1036612': 1.0, 'D1176370': 1.0, 'D1176864': 1.0, 'D179739': 1.0, 'D2368649': 1.0, 'D22916': 1.0})\n",
      "TOP 5 ['D2472276', 'D432626', 'D236010', 'D2536401', 'D2533657']\n",
      "D2472276\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "MODEL INDEXES [16, 3, 9, 4, 5]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D490432': 0.3342541754245758, 'D1504129': 0.2382058948278427, 'D402967': 0.20386865735054016, 'D1503515': 0.12463093549013138, 'D15017': 0.09904025495052338, 'D330646': 3.238577228039503, 'D330647': 0.09844931960105896, 'D330648': 0.16224191337823868, 'D328646': 0.042832594364881516, 'D330 646': 0.04270162060856819, 'D3504074': 0.07662069797515869, 'D2306411': 0.0755196288228035, 'D2904641': 0.06928057223558426, 'D330644': 0.053558219224214554, 'D1325390': 0.028743725270032883, 'D330326': 0.02828451804816723, 'D330643': 0.04207117110490799, 'D3208079': 0.04111894965171814})\n",
      "TOP 5 ['D330646', 'D490432', 'D1504129', 'D402967', 'D330648']\n",
      "D330646\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D490432': 1.0, 'D1504129': 1.0, 'D402967': 1.0, 'D1503515': 1.0, 'D15017': 1.0, 'D330646': 5.0, 'D330647': 2.0, 'D330648': 3.0, 'D328646': 1.0, 'D330 646': 1.0, 'D3504074': 1.0, 'D2306411': 1.0, 'D2904641': 1.0, 'D330644': 1.0, 'D1325390': 1.0, 'D330326': 1.0, 'D330643': 1.0, 'D3208079': 1.0})\n",
      "TOP 5 ['D330646', 'D330648', 'D330647', 'D490432', 'D1504129']\n",
      "D330646\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "MODEL INDEXES [9, 13, 15, 8, 1]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D1633918': 0.8819137662649155, 'D163367699': 0.16675060987472534, 'D1633684': 0.16116033494472504, 'D1633916': 0.5577969700098038, 'D1631884': 0.1373935490846634, 'D3053509': 0.23972240090370178, 'D1604717': 0.23263153433799744, 'D1019003': 0.18683983385562897, 'D581693': 0.17724667489528656, 'D58632': 0.16355952620506287, 'D3556339': 0.2216576188802719, 'D3524297': 0.20738016068935394, 'D3536345': 0.1976408213376999, 'D551156': 0.19489358365535736, 'D3362277': 0.1784278005361557, 'D1640148': 0.1890837401151657, 'D1650201': 0.1825968623161316, 'D1633524': 0.179623544216156, 'D1633924': 0.19851098954677582, 'D1633784': 0.18423371016979218, 'D1622864': 0.16093593835830688})\n",
      "TOP 5 ['D1633918', 'D1633916', 'D3053509', 'D1604717', 'D3556339']\n",
      "D1633918\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D1633918': 3.0, 'D163367699': 1.0, 'D1633684': 1.0, 'D1633916': 3.0, 'D1631884': 1.0, 'D3053509': 1.0, 'D1604717': 1.0, 'D1019003': 1.0, 'D581693': 1.0, 'D58632': 1.0, 'D3556339': 1.0, 'D3524297': 1.0, 'D3536345': 1.0, 'D551156': 1.0, 'D3362277': 1.0, 'D1640148': 1.0, 'D1650201': 1.0, 'D1633524': 1.0, 'D1633924': 1.0, 'D1633784': 1.0, 'D1622864': 1.0})\n",
      "TOP 5 ['D1633918', 'D1633916', 'D163367699', 'D1633684', 'D1631884']\n",
      "D1633918\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "MODEL INDEXES [10, 9, 2, 16, 14]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D1614025': 0.2565228343009949, 'D161331': 0.21350674331188202, 'D1814025': 0.18087127804756165, 'D161864': 0.17677399516105652, 'D1612038': 0.17232517898082733, 'D1502060': 0.8277753591537476, 'D1502055': 0.17621801793575287, 'D1505098': 0.16376644372940063, 'D1505060': 0.1569671928882599, 'D1502050': 0.15135592222213745, 'D174596': 0.17526137828826904, 'D2128922': 0.16791702806949615, 'D1502064': 0.09647946059703827, 'D2502098': 0.08425912261009216, 'D3298505': 0.2752701938152313, 'D1610378': 0.20634691417217255, 'D169875': 0.17865736782550812, 'D3124250': 0.17313745617866516, 'D164384': 0.16658802330493927, 'D1464777': 0.2443205714225769, 'D1444777': 0.22105005383491516, 'D1483190': 0.19880437850952148, 'D148549': 0.1830752044916153, 'D1189402': 0.15274974703788757})\n",
      "TOP 5 ['D1502060', 'D3298505', 'D1614025', 'D1464777', 'D1444777']\n",
      "D1502060\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D1614025': 1.0, 'D161331': 1.0, 'D1814025': 1.0, 'D161864': 1.0, 'D1612038': 1.0, 'D1502060': 2.0, 'D1502055': 1.0, 'D1505098': 1.0, 'D1505060': 1.0, 'D1502050': 1.0, 'D174596': 1.0, 'D2128922': 1.0, 'D1502064': 1.0, 'D2502098': 1.0, 'D3298505': 1.0, 'D1610378': 1.0, 'D169875': 1.0, 'D3124250': 1.0, 'D164384': 1.0, 'D1464777': 1.0, 'D1444777': 1.0, 'D1483190': 1.0, 'D148549': 1.0, 'D1189402': 1.0})\n",
      "TOP 5 ['D1502060', 'D1614025', 'D161331', 'D1814025', 'D161864']\n",
      "D1502060\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "MODEL INDEXES [13, 16, 5, 2, 19]\n",
      "\n",
      "RESULTS WITH PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D3389837': 0.2734152674674988, 'D2386047': 0.3778459429740906, 'D4266947': 0.17637167870998383, 'D2268910': 0.1731777787208557, 'D3260543': 0.1725575476884842, 'D2466206': 0.2617354393005371, 'D2490680': 0.22561664879322052, 'D1202469': 0.18880285322666168, 'D1203224': 0.17211031913757324, 'D241089': 0.15173479914665222, 'D731976': 1.3831801414489746, 'D731977': 0.08062134683132172, 'D731975': 0.06658599525690079, 'D723589': 0.06270426511764526, 'D1234404': 0.060185402631759644, 'D740568': 0.12894563376903534, 'D636346': 0.0852031409740448, 'D740449': 0.07771696895360947, 'D7407': 0.054857037961483, 'D2387019': 0.2956393361091614, 'D2389039': 0.18628261983394623, 'D2389019': 0.17619478702545166, 'D2369090': 0.16851505637168884})\n",
      "TOP 5 ['D731976', 'D2386047', 'D2387019', 'D3389837', 'D2466206']\n",
      "D731976\n",
      "True\n",
      "\n",
      "RESULTS WITHOUT PROBS\n",
      "ALL defaultdict(<class 'float'>, {'D3389837': 1.0, 'D2386047': 2.0, 'D4266947': 1.0, 'D2268910': 1.0, 'D3260543': 1.0, 'D2466206': 1.0, 'D2490680': 1.0, 'D1202469': 1.0, 'D1203224': 1.0, 'D241089': 1.0, 'D731976': 2.0, 'D731977': 1.0, 'D731975': 1.0, 'D723589': 1.0, 'D1234404': 1.0, 'D740568': 1.0, 'D636346': 1.0, 'D740449': 1.0, 'D7407': 1.0, 'D2387019': 1.0, 'D2389039': 1.0, 'D2389019': 1.0, 'D2369090': 1.0})\n",
      "TOP 5 ['D2386047', 'D731976', 'D3389837', 'D4266947', 'D2268910']\n",
      "D731976\n",
      "True\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 132\u001b[0m\n\u001b[1;32m    129\u001b[0m         threads\u001b[38;5;241m.\u001b[39mappend(thread)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m--> 132\u001b[0m     thread\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:1112\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:1132\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lock\u001b[38;5;241m.\u001b[39macquire(block, timeout):\n\u001b[1;32m   1133\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_list, train_df, test_df, tokenizer):\n",
    "        self.model_list = model_list\n",
    "        self.train_df = train_df.copy()\n",
    "        self.test_df = test_df.copy()\n",
    "        \n",
    "        print ('train set size:', self.train_df.shape[0])\n",
    "        print ('test set size:', self.test_df.shape[0])\n",
    "    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.counter = 0\n",
    "\n",
    "    def generate_text_beams(self, query):\n",
    "        self.counter += 1\n",
    "        if self.counter % 1000 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        results = defaultdict(float)\n",
    "        sampled_models = random.sample(self.model_list, 5)\n",
    "\n",
    "        for model in sampled_models:\n",
    "            # Ensure query is properly encoded\n",
    "            input_ids = self.tokenizer.encode(query, return_tensors='pt')\n",
    "            output = model.generate(input_ids, do_sample=False, return_dict_in_generate=True, output_scores=True,\n",
    "                                    num_beams=5, num_return_sequences=5)\n",
    "\n",
    "            beam_scores = output.sequences_scores\n",
    "            probabilities = softmax(beam_scores, dim=0).tolist()\n",
    "            model_res = [self.tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output.sequences]\n",
    "            \n",
    "            \n",
    "            for res, prob in zip(model_res, probabilities):\n",
    "                results[res] += prob\n",
    "\n",
    "        return self.top_5_generated_texts(results)\n",
    "\n",
    "    def top_5_generated_texts(self, input_dict):\n",
    "        sorted_responses = sorted(input_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [response[0] for response in sorted_responses[:5]]\n",
    "\n",
    "    def evaluate_accuracy(self):\n",
    "        self.train_df['generated_doc_id'] = self.train_df['query'].apply(self.generate_text_beams)\n",
    "        self.test_df['generated_doc_id'] = self.test_df['query'].apply(self.generate_text_beams)\n",
    "\n",
    "        acc_train = self.calculate_accuracy(self.train_df)\n",
    "        acc_test = self.calculate_accuracy(self.test_df)\n",
    "\n",
    "        return acc_train, acc_test\n",
    "\n",
    "    def calculate_accuracy(self, df):\n",
    "        return df.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / df.shape[0]\n",
    "\n",
    "# Assuming you have predefined dictionaries/lists for models and datasets, such as:\n",
    "# two_groups_list = [...]\n",
    "# three_groups_list = [...]\n",
    "# train_df_group1 = ...\n",
    "# test_df_group1 = ...\n",
    "# ... and so on for other groups\n",
    "# And a tokenizer instance\n",
    "\n",
    "def run_evaluation(group_nbr, model_list_nbr, tokenizer):\n",
    "    if model_list_nbr == 'two':\n",
    "        model_list = two_groups_list\n",
    "    elif model_list_nbr == 'three':\n",
    "        model_list = three_groups_list\n",
    "\n",
    "    train_df = globals()[f'train_df_group{group_nbr}']\n",
    "    test_df = globals()[f'test_df_group{group_nbr}']\n",
    "\n",
    "    manager = ModelManager(model_list, train_df, test_df, tokenizer)\n",
    "    \n",
    "    key = (group_nbr, model_list_nbr)\n",
    "    model_managers[key] = manager\n",
    "    \n",
    "    \n",
    "    acc_train, acc_test = manager.evaluate_accuracy()\n",
    "    global_accuracies[key] = {'acc_train': acc_train, 'acc_test': acc_test}\n",
    "\n",
    "    print(f\"Group: {group_nbr}, Model List: {model_list_nbr}, Train Acc: {acc_train}, Test Acc: {acc_test}\")\n",
    "\n",
    "    \n",
    "# Global collection to store ModelManager instances\n",
    "model_managers = {}\n",
    "global_accuracies = {}\n",
    "    \n",
    "# Threading\n",
    "threads = []\n",
    "\n",
    "for group_nbr in range(1, 4):\n",
    "    for model_list_nbr in ['two', 'three']:\n",
    "        if group_nbr == 3 and model_list_nbr == 'two':\n",
    "            continue\n",
    "        thread = threading.Thread(target=run_evaluation, args=(group_nbr, model_list_nbr, tokenizer))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa73a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70580685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('global_accuracies_samplingmodels_5beams_probabilities.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('global_accuracies_samplingmodels_5beams_probabilities.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c24dff",
   "metadata": {},
   "source": [
    "# Sampling random models and aggregating their suggestions - 1 beams, with probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e9435",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_groups_list = []\n",
    "three_groups_list = []\n",
    "for group in groups:\n",
    "    for peer in peers:\n",
    "        exec(f'three_groups_list.append(model_group{group}_peer{int(peer)-8089})')\n",
    "        if int(group)<3:\n",
    "            exec(f'two_groups_list.append(model_group{group}_peer{int(peer)-8089})')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a72e19a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_list, train_df, test_df, tokenizer):\n",
    "        self.model_list = model_list\n",
    "        self.train_df = train_df.copy()\n",
    "        self.test_df = test_df.copy()\n",
    "        \n",
    "        print ('train set size:', self.train_df.shape[0])\n",
    "        print ('test set size:', self.test_df.shape[0])\n",
    "    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.counter = 0\n",
    "\n",
    "    def generate_text_beams(self, query):\n",
    "        self.counter += 1\n",
    "        if self.counter % 1000 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        results = defaultdict(float)\n",
    "        sampled_models = random.sample(self.model_list, 5)\n",
    "\n",
    "        for model in sampled_models:\n",
    "            # Ensure query is properly encoded\n",
    "            input_ids = self.tokenizer.encode(query, return_tensors='pt')\n",
    "            output = model.generate(input_ids, do_sample=False, return_dict_in_generate=True, output_scores=True,\n",
    "                                    num_beams=5, num_return_sequences=5)\n",
    "\n",
    "            beam_scores = output.sequences_scores\n",
    "            probabilities = softmax(beam_scores, dim=0).tolist()\n",
    "            model_res = [self.tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output.sequences]\n",
    "            \n",
    "            \n",
    "            for res, prob in zip(model_res, probabilities):\n",
    "                results[res] += prob\n",
    "\n",
    "        return self.top_5_generated_texts(results)\n",
    "\n",
    "    def top_5_generated_texts(self, input_dict):\n",
    "        sorted_responses = sorted(input_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [response[0] for response in sorted_responses[:1]]\n",
    "\n",
    "    def evaluate_accuracy(self):\n",
    "        self.train_df['generated_doc_id'] = self.train_df['query'].apply(self.generate_text_beams)\n",
    "        self.test_df['generated_doc_id'] = self.test_df['query'].apply(self.generate_text_beams)\n",
    "\n",
    "        acc_train = self.calculate_accuracy(self.train_df)\n",
    "        acc_test = self.calculate_accuracy(self.test_df)\n",
    "\n",
    "        return acc_train, acc_test\n",
    "\n",
    "    def calculate_accuracy(self, df):\n",
    "        return df.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / df.shape[0]\n",
    "\n",
    "# Assuming you have predefined dictionaries/lists for models and datasets, such as:\n",
    "# two_groups_list = [...]\n",
    "# three_groups_list = [...]\n",
    "# train_df_group1 = ...\n",
    "# test_df_group1 = ...\n",
    "# ... and so on for other groups\n",
    "# And a tokenizer instance\n",
    "\n",
    "def run_evaluation(group_nbr, model_list_nbr, tokenizer):\n",
    "    if model_list_nbr == 'two':\n",
    "        model_list = two_groups_list\n",
    "    elif model_list_nbr == 'three':\n",
    "        model_list = three_groups_list\n",
    "\n",
    "    train_df = globals()[f'train_df_group{group_nbr}']\n",
    "    test_df = globals()[f'test_df_group{group_nbr}']\n",
    "\n",
    "    manager = ModelManager(model_list, train_df, test_df, tokenizer)\n",
    "    \n",
    "    key = (group_nbr, model_list_nbr)\n",
    "    model_managers[key] = manager\n",
    "    \n",
    "    \n",
    "    acc_train, acc_test = manager.evaluate_accuracy()\n",
    "    global_accuracies[key] = {'acc_train': acc_train, 'acc_test': acc_test}\n",
    "\n",
    "    print(f\"Group: {group_nbr}, Model List: {model_list_nbr}, Train Acc: {acc_train}, Test Acc: {acc_test}\")\n",
    "\n",
    "    \n",
    "# Global collection to store ModelManager instances\n",
    "model_managers = {}\n",
    "global_accuracies = {}\n",
    "    \n",
    "# Threading\n",
    "threads = []\n",
    "\n",
    "for group_nbr in range(1, 4):\n",
    "    for model_list_nbr in ['two', 'three']:\n",
    "        if group_nbr == 3 and model_list_nbr == 'two':\n",
    "            continue\n",
    "        thread = threading.Thread(target=run_evaluation, args=(group_nbr, model_list_nbr, tokenizer))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b258cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('global_accuracies_samplingmodels_probabilities.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34790e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('global_accuracies_samplingmodels_probabilities.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf6673",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb3d382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7b0db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
