[DEFAULT]
# Output is either a single sigmoid neuron (True) 
# or a softmax layer with two neurons (False)
SingleOutput = False

# Loss function must be compatible with output layer (cf. SingleOutput)
# Refer to https://pytorch.org/docs/stable/nn.html#loss-functions
LossFunction = CrossEntropyLoss

# Typically SGD or Adam
# Refer to https://pytorch.org/docs/stable/optim.html#algorithms
Optimizer = AdamW

# Epochs per unit of training data (will be used as a multiplier)
EpochScale = 1

# Learning rate for the optimizer
LearningRate  = 0.0007

# Number of hidden layers and units per layer
HiddenLayers = 2
HiddenUnits = 1024

# Regularization through dropout to prevent overfitting (set 0 to disable)
Dropout = 0.3

# Enable quantization-aware training and quantized model exchange
Quantize = False

# Number of results to return on each query
NumberOfResults = 10